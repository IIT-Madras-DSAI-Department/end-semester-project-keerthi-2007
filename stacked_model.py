# -*- coding: utf-8 -*-
"""Stacked model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bwjsMtm-HyR78uD1bS3lYM7G29zZqICo
"""

import numpy as np
import time
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score

dftrain = pd.read_csv('/content/MNIST_train.csv')
dfval = pd.read_csv('/content/MNIST_validation.csv')

dftrain = dftrain.drop('even', axis=1)

dfval = dfval.drop('even', axis=1)

featurecols = list(dftrain.columns)
targetcol = 'label'
featurecols.remove(targetcol)

print('length of featurecolumns is', len(featurecols))

Xtrain = np.array(dftrain[featurecols]) / 255
ytrain = np.array(dftrain[targetcol])

Xval = np.array(dfval[featurecols]) / 255
yval = np.array(dfval[targetcol])

class PCAModel:
    def __init__(self, n_components):
        self.n_components = n_components
        self.mean = None
        self.components = None

    def fit(self, X):
        X = np.array(X, dtype=float)
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean
        cov_matrix = np.cov(X_centered, rowvar=False)
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        sorted_idx = np.argsort(eigenvalues)[::-1]
        self.components = eigenvectors[:, sorted_idx][:, :self.n_components]

    def transform(self, X):
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)

class KNNClassifier:
    def __init__(self, k=5):
        self.k = k
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        self.X_train = np.array(X)
        self.y_train = np.array(y)

    def predict(self, X):
        X = np.array(X)
        distances = np.sqrt(((X[:, None, :] - self.X_train[None, :, :]) ** 2).sum(axis=2))
        nn_idx = np.argsort(distances, axis=1)[:, :self.k]
        nn_labels = self.y_train[nn_idx]
        y_pred = np.array([np.bincount(row, minlength=self.y_train.max() + 1).argmax() for row in nn_labels])
        return y_pred

    def predict_proba(self, X):
        X = np.array(X)
        distances = np.sqrt(((X[:, None, :] - self.X_train[None, :, :]) ** 2).sum(axis=2))
        nn_idx = np.argsort(distances, axis=1)[:, :self.k]
        nn_labels = self.y_train[nn_idx]
        probs = np.array([np.bincount(row, minlength=self.y_train.max() + 1) / self.k for row in nn_labels])
        return probs

class KNNWithCustomPCA:
    def __init__(self, k=5, n_components=None):
        self.k = k
        self.n_components = n_components
        self.pca = None
        self.knn = KNNClassifier(k=k)

    def fit(self, X, y):
        X = np.asarray(X, dtype=float)
        self.pca = PCAModel(n_components=self.n_components)
        self.pca.fit(X)
        X_reduced = self.pca.transform(X)
        self.knn.fit(X_reduced, y)

    def predict(self, X):
        X_reduced = self.pca.transform(np.asarray(X, dtype=float))
        return self.knn.predict(X_reduced)

    def predict_proba(self, X):
        X_reduced = self.pca.transform(np.asarray(X, dtype=float))
        return self.knn.predict_proba(X_reduced)

class tree_node:
    def __init__(self, feature_idx=None, threshold=None, left_node=None, right_node=None, leaf_value=None):
        self.feature_idx = feature_idx
        self.threshold = threshold
        self.left_node = left_node
        self.right_node = right_node
        self.leaf_value = leaf_value

class XGBoostMultiClass:
    def __init__(self, num_classes, n_estimators=100, learning_rate=0.1, max_depth=3):
        self.K = num_classes
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.trees = []

    def softmax(self, logits):
        logits = logits - np.max(logits, axis=1, keepdims=True)
        exp_vals = np.exp(logits)
        return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)

    def build(self, X, grad, hess, depth):
        if depth >= self.max_depth or len(X) == 0:
            leaf_value = -np.sum(grad) / (np.sum(hess) + 1e-8)
            return tree_node(leaf_value=leaf_value)

        G_total, H_total = np.sum(grad), np.sum(hess)
        best_gain = -float('inf')
        best_feat = None
        best_thresh = None
        best_left = best_right = None

        feature_count = int(np.sqrt(X.shape[1])) + 1
        feature_indices = np.random.choice(X.shape[1], feature_count, replace=False)

        for j in feature_indices:
            values = np.unique(X[:, j])
            thresholds = np.random.choice(values, min(10, len(values)), replace=False)
            for threshold in thresholds:
                left = X[:, j] <= threshold
                right = ~left
                if not np.any(left) or not np.any(right):
                    continue
                G_l, H_l = np.sum(grad[left]), np.sum(hess[left])
                G_r, H_r = np.sum(grad[right]), np.sum(hess[right])
                gain = 0.5 * (G_l**2 / (H_l + 1e-8) + G_r**2 / (H_r + 1e-8) - G_total**2 / (H_total + 1e-8))
                if gain > best_gain:
                    best_gain = gain
                    best_feat = j
                    best_thresh = threshold
                    best_left = left
                    best_right = right

        if best_gain == -float('inf'):
            leaf_value = -np.sum(grad) / (np.sum(hess) + 1e-8)
            return tree_node(leaf_value=leaf_value)

        left_node = self.build(X[best_left],  grad[best_left],  hess[best_left],  depth+1)
        right_node = self.build(X[best_right], grad[best_right], hess[best_right], depth+1)
        return tree_node(feature_idx=best_feat, threshold=best_thresh, left_node=left_node, right_node=right_node)

    def pred_one(self, x, node):
        while node.leaf_value is None:
            node = node.left_node if x[node.feature_idx] <= node.threshold else node.right_node
        return node.leaf_value

    def fit(self, X, y):
        X = np.asarray(X)
        y = np.asarray(y)
        N = len(y)
        Y = np.eye(self.K)[y]
        scores = np.zeros((N, self.K))
        self.trees = []

        for m in range(self.n_estimators):
            probs = self.softmax(scores)
            grad = probs - Y
            hess = probs * (1 - probs)
            round_trees = []

            for k in range(self.K):
                tree = self.build(X, grad[:, k], hess[:, k], depth=0)
                update = np.array([self.pred_one(row, tree) for row in X])
                scores[:, k] += self.learning_rate * update
                round_trees.append(tree)

            self.trees.append(round_trees)

    def predict(self, X):
        X = np.asarray(X)
        scores = np.zeros((X.shape[0], self.K))
        for round_trees in self.trees:
            for k in range(self.K):
                update = np.array([self.pred_one(row, round_trees[k]) for row in X])
                scores[:, k] += self.learning_rate * update
        return np.argmax(self.softmax(scores), axis=1)

    def predict_proba(self, X):
        X = np.asarray(X)
        scores = np.zeros((X.shape[0], self.K))
        for round_trees in self.trees:
            for k in range(self.K):
                update = np.array([self.pred_one(row, round_trees[k]) for row in X])
                scores[:, k] += self.learning_rate * update
        return self.softmax(scores)

class StackedClassifier:
    def __init__(self, base_models):
        self.base_models = base_models
        self.meta_model = None
        self.base_models_fitted = []

    def fit(self, X, y):
        X = np.asarray(X)
        y = np.asarray(y)
        N = X.shape[0]
        for model in self.base_models:
            model.fit(X, y)
            self.base_models_fitted.append(model)

    def predict(self, X):
        X = np.asarray(X)
        base_preds = np.array([model.predict(X) for model in self.base_models_fitted])
        y_pred = np.array([np.bincount(base_preds[:, i]).argmax() for i in range(X.shape[0])])
        return y_pred

    def predict_proba(self, X):
        X = np.asarray(X)
        probs_list = [model.predict_proba(X) for model in self.base_models_fitted]
        avg_probs = np.mean(probs_list, axis=0)
        return avg_probs

num_classes = len(np.unique(ytrain))

knn_model = KNNWithCustomPCA(k=5, n_components=100)
xgb_model = XGBoostMultiClass(num_classes=num_classes, n_estimators=100, learning_rate=0.1, max_depth=3)

stacked_model = StackedClassifier(base_models=[knn_model, xgb_model])

start_time = time.time()
stacked_model.fit(Xtrain, ytrain)
training_time = time.time() - start_time
y_train_pred = stacked_model.predict(Xtrain)
y_val_pred = stacked_model.predict(Xval)
train_acc = accuracy_score(ytrain, y_train_pred)
val_acc = accuracy_score(yval, y_val_pred)
train_f1 = f1_score(ytrain, y_train_pred, average='weighted')
val_f1 = f1_score(yval, y_val_pred, average='weighted')

print("----------------------------")
print("STACKED MODEL (KNN + PCA + XGBoost) RESULTS")
print("----------------------------")
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}")
print(f"Train F1: {train_f1:.4f}")
print(f"Validation F1: {val_f1:.4f}")
print(f"Training time: {training_time:.2f} seconds")
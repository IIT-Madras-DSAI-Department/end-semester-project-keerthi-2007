# -*- coding: utf-8 -*-
"""XGboost_Hyp_PCA .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IUfSe4wLWgLKHcHPX95MzJcbef3-n1zG
"""

import numpy as np
import time
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score

dftrain = pd.read_csv('/content/MNIST_train.csv')
dfval = pd.read_csv('/content/MNIST_validation.csv')

dftrain = dftrain.drop('even', axis=1)

dfval = dfval.drop('even', axis=1)

featurecols = list(dftrain.columns)
targetcol = 'label'
featurecols.remove(targetcol)

print('length of featurecolumns is', len(featurecols))

Xtrain = np.array(dftrain[featurecols]) / 255
ytrain = np.array(dftrain[targetcol])

Xval = np.array(dfval[featurecols]) / 255
yval = np.array(dfval[targetcol])

class PCAModel:
    def __init__(self, n_components):
        self.n_components = n_components
        self.mean = None
        self.components = None
        self.explained_variance = None

    def fit(self, X):
        X = np.array(X, dtype=float)

        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean

        cov_matrix = np.cov(X_centered, rowvar=False)
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

        sorted_idx = np.argsort(eigenvalues)[::-1]

        self.explained_variance = eigenvalues[sorted_idx][:self.n_components]
        self.components = eigenvectors[:, sorted_idx][:, :self.n_components]

    def predict(self, X):
        if self.mean is None:
            raise ValueError("PCA is not fitted yet.")

        X_centered = X - self.mean
        return np.dot(X_centered, self.components)

    def reconstruct(self, X):
        Z = self.predict(X)
        return np.dot(Z, self.components.T) + self.mean

class tree_node:
    def __init__(self, feature_idx=None, threshold=None, left_node=None, right_node=None, leaf_value=None):
        self.feature_idx = feature_idx
        self.threshold = threshold
        self.left_node = left_node
        self.right_node = right_node
        self.leaf_value = leaf_value

class XGBoostMultiClass:
    def __init__(self, num_classes, n_estimators=200, learning_rate=0.2, max_depth=5):
        self.K = num_classes
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.trees = []
        self.init_scores = None

    def softmax(self, logits):
        logits = logits - np.max(logits, axis=1, keepdims=True)
        exp_vals = np.exp(logits)
        return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)

    def build(self, X, grad, hess, depth):
        if depth >= self.max_depth or len(X) == 0:
            leaf_value = -np.sum(grad) / (np.sum(hess) + 1e-8)
            return tree_node(leaf_value=leaf_value)

        G_total, H_total = np.sum(grad), np.sum(hess)
        best_gain = -float('inf')
        best_feat = None
        best_thresh = None
        best_left = best_right = None

        feature_count = int(np.sqrt(X.shape[1])) + 1
        feature_indices = np.random.choice(X.shape[1], feature_count, replace=False)

        for j in feature_indices:
            values = np.unique(X[:, j])
            thresholds = np.random.choice(values, min(10, len(values)), replace=False)

            for threshold in thresholds:
                left = X[:, j] <= threshold
                right = ~left

                if not np.any(left) or not np.any(right):
                    continue

                G_l, H_l = np.sum(grad[left]), np.sum(hess[left])
                G_r, H_r = np.sum(grad[right]), np.sum(hess[right])

                gain = 0.5 * (
                    G_l**2 / (H_l + 1e-8) +
                    G_r**2 / (H_r + 1e-8) -
                    G_total**2 / (H_total + 1e-8)
                )

                if gain > best_gain:
                    best_gain = gain
                    best_feat = j
                    best_thresh = threshold
                    best_left = left
                    best_right = right

        if best_gain == -float('inf'):
            leaf_value = -np.sum(grad) / (np.sum(hess) + 1e-8)
            return tree_node(leaf_value=leaf_value)

        left_node = self.build(X[best_left],  grad[best_left],  hess[best_left],  depth+1)
        right_node = self.build(X[best_right], grad[best_right], hess[best_right], depth+1)

        return tree_node(feature_idx=best_feat, threshold=best_thresh,
                         left_node=left_node, right_node=right_node)

    def pred_one(self, x, node):
        while node.leaf_value is None:
            if x[node.feature_idx] <= node.threshold:
                node = node.left_node
            else:
                node = node.right_node
        return node.leaf_value
    def fit(self, X, y):
        X = np.asarray(X)
        y = np.asarray(y)
        N = len(y)

        Y = np.eye(self.K)[y]

        scores = np.zeros((N, self.K))
        self.init_scores = np.zeros(self.K)

        self.trees = []

        for m in range(self.n_estimators):
            probs = self.softmax(scores)
            grad = probs - Y
            hess = probs * (1 - probs)

            round_trees = []
            for k in range(self.K):
                tree = self.build(X, grad[:, k], hess[:, k], depth=0)
                update = np.array([self.pred_one(row, tree) for row in X])
                scores[:, k] += self.learning_rate * update

                round_trees.append(tree)

            self.trees.append(round_trees)
    def predict(self, X):
        X = np.asarray(X)
        N = X.shape[0]

        scores = np.zeros((N, self.K))

        for round_trees in self.trees:
            for k in range(self.K):
                update = np.array([self.pred_one(row, round_trees[k]) for row in X])
                scores[:, k] += self.learning_rate * update

        probs = self.softmax(scores)
        return np.argmax(probs, axis=1)

    def predict_proba(self, X):
        X = np.asarray(X)
        N = X.shape[0]

        scores = np.zeros((N, self.K))

        for round_trees in self.trees:
            for k in range(self.K):
                update = np.array([self.pred_one(row, round_trees[k]) for row in X])
                scores[:, k] += self.learning_rate * update

        return self.softmax(scores)

num_classes = len(np.unique(ytrain))
pca = PCAModel(n_components=100)
pca.fit(Xtrain)
X_train_pca = pca.predict(Xtrain)
X_val_pca = pca.predict(Xval)
xgb = XGBoostMultiClass(num_classes=num_classes, n_estimators=50, learning_rate=0.1, max_depth=3)
start_time = time.time()
xgb.fit(X_train_pca, ytrain)
training_time = time.time() - start_time
y_train_pred = xgb.predict(X_train_pca)
y_val_pred = xgb.predict(X_val_pca)
train_acc = accuracy_score(ytrain, y_train_pred)
val_acc = accuracy_score(yval, y_val_pred)
train_f1 = f1_score(ytrain, y_train_pred, average='weighted')
val_f1 = f1_score(yval, y_val_pred, average='weighted')

print("----------------------------")
print("XGBoost (Multi-class) + PCA RESULTS")
print("----------------------------")
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}")
print(f"Train F1: {train_f1:.4f}")
print(f"Validation F1: {val_f1:.4f}")
print(f"Training time: {training_time:.2f} seconds")

num_classes = len(np.unique(ytrain))
components=[50,100,200,300]
for component in components:
    pca_c = PCAModel(n_components=100)
    pca_c.fit(Xtrain)
    X_train_pca = pca_c.predict(Xtrain)
    X_val_pca = pca_c.predict(Xval)
    xgb = XGBoostMultiClass(num_classes=num_classes, n_estimators=50, learning_rate=0.1, max_depth=3)
    start_time = time.time()
    xgb.fit(X_train_pca, ytrain)
    training_time = time.time() - start_time
    y_train_pred = xgb.predict(X_train_pca)
    y_val_pred = xgb.predict(X_val_pca)
    train_acc = accuracy_score(ytrain, y_train_pred)
    val_acc = accuracy_score(yval, y_val_pred)
    train_f1 = f1_score(ytrain, y_train_pred, average='weighted')
    val_f1 = f1_score(yval, y_val_pred, average='weighted')

    print("----------------------------")
    print("XGBoost (Multi-class) + PCA RESULTS")
    print("----------------------------")
    print(f"Train Accuracy: {train_acc:.4f}")
    print(f"Validation Accuracy: {val_acc:.4f}")
    print(f"Train F1: {train_f1:.4f}")
    print(f"Validation F1: {val_f1:.4f}")
    print(f"Training time: {training_time:.2f} seconds")

estimators=[100,150,200,500]
for estimator in estimators:
  pca = PCAModel(n_components=100)
  pca.fit(Xtrain)
  X_train_pca = pca.predict(Xtrain)
  X_val_pca = pca.predict(Xval)
  xgb_e = XGBoostMultiClass(num_classes=num_classes, n_estimators=estimator, learning_rate=0.1, max_depth=3)
  start_time_e = time.time()
  xgb_e.fit(X_train_pca, ytrain)
  training_time = time.time()-start_time_e
  y_train_pred_e = xgb_e.predict(X_train_pca)
  y_val_pred_e = xgb_e.predict(X_val_pca)
  train_acc_e = accuracy_score(ytrain, y_train_pred_e)
  val_acc_e = accuracy_score(yval, y_val_pred_e)
  train_f1_e = f1_score(ytrain, y_train_pred_e, average='weighted')
  val_f1_e = f1_score(yval, y_val_pred_e, average='weighted')
  print("----------------------------")
  print("XGBoost (Multi-class) + PCA RESULTS")
  print("----------------------------")
  print(f"Train Accuracy: {train_acc_e:.4f}")
  print(f"Validation Accuracy: {val_acc_e:.4f}")
  print(f"Train F1: {train_f1_e:.4f}")
  print(f"Validation F1: {val_f1_e:.4f}")
  print(f"Training time: {training_time:.2f} seconds")
# -*- coding: utf-8 -*-
"""Multinomial Logistic Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fOC6VKcbU3J0WCKgzBqf1mnnsn2dKfrl
"""

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt

df=pd.read_csv("/content/MNIST_train.csv")

df=df.drop("even",axis=1, errors='ignore')
X=df.drop("label", axis=1)
y=df["label"]

X=X/255

class SoftmaxRegression:
    def __init__(self, learning_rate=0.1, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.W = None    #weights is a 2D vector
        self.b = None    # bias is also a 2D vector

    def _softmax(self, z):
        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability trick
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)

    def _one_hot(self, y, num_classes):
        return np.eye(num_classes)[y]

    def _cross_entropy_loss(self, y_true, y_pred):
        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))

    def fit(self, X, y):
        num_samples, num_features = X.shape
        num_classes = np.max(y) + 1  # assuming labels are 0-indexed

        # Initialize weights and bias
        self.W = np.random.randn(num_features, num_classes) * 0.01
        self.b = np.zeros((1, num_classes))
        #print ('shape of weights is', np.shape(self.W))
        #print ('shape of bias is', np.shape(self.b))

        # One-hot encode labels
        Y_onehot = self._one_hot(y, num_classes)
        #print ('shape of label vector is', np.shape(Y_onehot))

        for epoch in range(self.epochs):
            # Forward pass
            logits = np.dot(X, self.W) + self.b
            probs = self._softmax(logits)
            #print ('shape of logits is', np.shape(logits))
            #print ('shape of probs is', np.shape(probs))

            # Loss (for monitoring)
            loss = self._cross_entropy_loss(Y_onehot, probs)
            #print ('shape of loss vector is', np.shape(loss))

            # Backward pass
            grad_logits = (1./ num_samples) * (Y_onehot - probs)
            grad_W = -np.dot(X.T, grad_logits)
            grad_b = -np.sum(grad_logits, axis=0, keepdims=True)

            # Update weights
            self.W -= self.learning_rate * grad_W
            self.b -= self.learning_rate * grad_b

            if epoch % 100 == 0 or epoch == self.epochs - 1:
                print(f"Epoch {epoch}: Loss = {loss:.4f}")

    def predict_proba(self, X):
        logits = np.dot(X, self.W) + self.b
        return self._softmax(logits)

    def predict(self, X):
        probs = self.predict_proba(X)
        return np.argmax(probs, axis=1)

df_val=pd.read_csv('/content/MNIST_validation.csv')

df_val=df_val.drop("even",axis=1, errors='ignore')
X_val=df_val.drop("label",axis=1)
y_val=df_val["label"]

X_val=X_val/255

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from sklearn.metrics import f1_score, confusion_matrix

import time
start=time.time()
model=SoftmaxRegression()
model.fit(X, y)
ypred_train=model.predict(X.values)
ypred=model.predict(X_val.values)
log_acc=accuracy_score(y_val, ypred)
log_acc_train=accuracy_score(y, ypred_train)
weighted_f1 = f1_score(y_val, ypred, average='weighted')
weighted_f1_train=f1_score(y, ypred_train, average='weighted')
print("--- %s seconds ---" % (time.time() - start))
print(f"Logistic Regression  Accuracy: {log_acc:.4f}")
print("Weighted F1 score:", weighted_f1)
print(f"Logistic Regression  Accuracy train: {log_acc_train:.4f}")
print("Weighted F1 score:-train", weighted_f1_train)

print(ypred)

ConfusionMatrixDisplay(confusion_matrix(y_val, ypred)).plot()

"""Hyperparamter Tuning"""

epochs=[100,200,500,1000,2000]
for e in epochs:
  start_e=time.time()
  model_e=SoftmaxRegression(epochs=e)
  model_e.fit(X, y)
  ypred_train_e=model_e.predict(X.values)
  ypred_e=model_e.predict(X_val.values)
  log_acc_train_e=accuracy_score(y, ypred_train_e)
  log_acc_e=accuracy_score(y_val, ypred_e)
  weighted_f1_train_e = f1_score(y, ypred_train_e, average='weighted')
  weighted_f1_e = f1_score(y_val, ypred_e, average='weighted')
  print(f"--- (epoch-{e}) {time.time() - start_e:.2f} seconds ---")
  print(f"Logistic Regression  Accuracy train-{e}: {log_acc_train_e:.4f}")
  print(f"Logistic Regression  Accuracy-{e}: {log_acc_e:.4f}")
  print(f"Weighted F1 score-{e}: {weighted_f1_e}")
  print(f"Weighted F1 score train-{e}: {weighted_f1_train_e}")
  print('-' * 50)

lr=[0.01,0.02,0.1,0.2]
for l in lr:
  start_l=time.time()
  model_l=SoftmaxRegression(learning_rate=l)
  model_l.fit(X, y)
  ypred_train_l=model_l.predict(X.values)
  ypred_l=model_l.predict(X_val.values)
  log_acc_train_l=accuracy_score(y, ypred_train_l)
  log_acc_l=accuracy_score(y_val, ypred_l)
  weighted_f1_train_l = f1_score(y, ypred_train_l, average='weighted')
  weighted_f1_l = f1_score(y_val, ypred_l, average='weighted')
  print(f"--- (Learning rate-{l}) {time.time() - start_l:.2f} seconds ---")
  print(f"Logistic Regression  Accuracy train-{l}: {log_acc_train_l:.4f}")
  print(f"Logistic Regression  Accuracy-{l}: {log_acc_l:.4f}")
  print(f"Weighted F1 score train-{l}: {weighted_f1_train_l}")
  print(f"Weighted F1 score-{l}: {weighted_f1_l}")
  print('-' * 50)

import time
start_best=time.time()
model_best=SoftmaxRegression(learning_rate=0.2,epochs=2000)
model_best.fit(X, y)
ypred_train=model_best.predict(X.values)
ypred_best=model_best.predict(X_val.values)
log_acc_train_best=accuracy_score(y, ypred_train)
log_acc_best=accuracy_score(y_val, ypred_best)
weighted_f1_train_best=f1_score(y, ypred_train, average='weighted')
weighted_f1_best = f1_score(y_val, ypred_best, average='weighted')
print("--- %s seconds ---" % (time.time() - start_best))
print(f"Logistic Regression  Accuracy best: {log_acc_best:.4f}")
print("Weighted F1 score:(best)", weighted_f1_best)
print(f"Logistic Regression  Accuracy train: {log_acc_train_best:.4f}")
print("Weighted F1 score:(train)", weighted_f1_train_best)